<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>Statistics</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="The non-Tech Second Brain of Kyle Tycholiz"/><meta property="og:title" content="Statistics"/><meta property="og:description" content="The non-Tech Second Brain of Kyle Tycholiz"/><meta property="og:url" content="https://thoughts.kyletycholiz.com/notes/kh7Y1bQw7Om31yurE4CrW/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="3/28/2021"/><meta property="article:modified_time" content="11/12/2021"/><link rel="canonical" href="https://thoughts.kyletycholiz.com/notes/kh7Y1bQw7Om31yurE4CrW/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6b338472289fe290.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/Ev72apsD0Krsz6dJLOqfG/_buildManifest.js" defer=""></script><script src="/_next/static/Ev72apsD0Krsz6dJLOqfG/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="statistics">Statistics<a aria-hidden="true" class="anchor-heading icon-link" href="#statistics"></a></h1>
<p>A person who doesn't stop to consider the assumptions of the techniques she's using is, in effect, thinking that her techniques are magical</p>
<h1 id="overview">Overview<a aria-hidden="true" class="anchor-heading icon-link" href="#overview"></a></h1>
<h2 id="approaches">Approaches<a aria-hidden="true" class="anchor-heading icon-link" href="#approaches"></a></h2>
<h3 id="descriptive-statistics">Descriptive Statistics<a aria-hidden="true" class="anchor-heading icon-link" href="#descriptive-statistics"></a></h3>
<ul>
<li>a summary of data that quantitatively describes or summarizes features from a collection of information</li>
<li>aim is to summarize a sample
<ul>
<li>It is therefore parametric
<ul>
<li>ie. the analysis of the statistical set is based on parameters, like <code>variance</code>, or a <code>central tendency</code> (ie. mean, median, mode, range)
<ul>
<li>a central tendency is often contrasted with its own level of variance</li>
<li>both central tendencies and their levels of variance are considered main properties of a distribution. With them we may judge whether a set of data has a strong central tendency based on the level of variance that it has.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>ex. <em>mean</em>, <em>standard deviation</em></li>
<li>spec: description statistics are backward looking, inductive statistics are forward looking</li>
<li>descriptive statistics does not rest on the assumption that the data came from a larger population</li>
</ul>
<h3 id="inferential-inductive-statistics">Inferential (Inductive) Statistics<a aria-hidden="true" class="anchor-heading icon-link" href="#inferential-inductive-statistics"></a></h3>
<ul>
<li>aim is to learn about the population that the sample represents
<ul>
<li>Therefore, it is developed on the basis of probability theory and by definition, cannot be parametric</li>
</ul>
</li>
<li>Statistical inference is the process of analyzing data to make inferences on an underlying</li>
<li>Descriptive stats do add valuable context when trying to analyze data inductively. 
<ul>
<li>ex. in papers reporting on human subjects, typically a table is included giving the overall sample size, sample sizes in important subgroups, as well as demographic info like age, sex, ethnicity</li>
</ul>
</li>
<li>The conclusion of a statistical inference is called a <code>statistical proposition</code></li>
</ul>
<h2 id="probability-distribution">Probability Distribution<a aria-hidden="true" class="anchor-heading icon-link" href="#probability-distribution"></a></h2>
<ul>
<li>def - the mathematical function that tells us the probability that each possible outcome occurs. </li>
<li>It is a mathematical representation of a random phenomenon</li>
<li>It is a function which takes the <code>sample space</code> (ie. all possible outcomes) as an input, and returns us the probability of their occurrence.
<ul>
<li>ex. of a coin flip, <code>sample space = {HEADS, TAILS}</code></li>
<li>In other words, we give the function a single possible outcome, and the graph gives us the probability of that outcome occurring (the outcome of an event occurring is a subset of the sample space.</li>
</ul>
</li>
<li>the probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory</li>
<li>Probability distributions are more preferable than simple numbers for describing a quantity because:
<ol>
<li>There is spread or variability in almost any value that can be measured in a population</li>
<li>almost all measurements are made with some intrinsic error
<ul>
<li>ex. in physics many processes are described probabilistically</li>
</ul>
</li>
</ol>
</li>
<li><a href="https://en.wikipedia.org/wiki/List_of_probability_distributions">Common probability distributions</a>:
<ul>
<li>normal distribution, which is related to linear growth</li>
<li>pareto distribution (pareto principle), which is related to exponential growth</li>
</ul>
</li>
</ul>
<h3 id="discrete-probability-distribution">Discrete Probability Distribution<a aria-hidden="true" class="anchor-heading icon-link" href="#discrete-probability-distribution"></a></h3>
<ul>
<li>All binomial distributions are discrete
<ul>
<li>in other words, they are composed of discrete random variables
<ul>
<li>variable here, refers to the potential outcomes (observations) that can occur</li>
</ul>
</li>
</ul>
</li>
<li>The set of possible outcomes is discrete and predictable
<ul>
<li>ex. coin toss, dice roll</li>
</ul>
</li>
</ul>
<h3 id="continuous-probability-distribution">Continuous Probability Distribution<a aria-hidden="true" class="anchor-heading icon-link" href="#continuous-probability-distribution"></a></h3>
<ul>
<li>All normal distribtions are continuous</li>
<li>The set of possible outcomes can take place on a continuous range
<ul>
<li>ex. temperatures</li>
</ul>
</li>
<li>These probabilities are described by a <code>probability density function</code> (PDF)
<ul>
<li>This function takes in a single sample from the <code>sample space</code> and returns us "the probability that the input variable would equal that sample in real life"</li>
<li>ex. what is the probability that the temperature at 7:00PM, as measured in reality, is equal to the sample recorded in this sample?</li>
</ul>
</li>
<li>the absolute likelihood for a continuous random variable to take on any particular value is 0
<ul>
<li>This is because there are an infinite set of possible values (ie. infinite sample space)</li>
</ul>
</li>
</ul>
<h3 id="univariatemultivariate-distribution">Univariate/Multivariate Distribution<a aria-hidden="true" class="anchor-heading icon-link" href="#univariatemultivariate-distribution"></a></h3>
<ul>
<li>Univariate - The probability distribution of only 1 random variable</li>
<li>Multivariate - The probability distribution of multiple random variables (a vector)
<ul>
<li>regression towards the mean can be defined for any bivariate distribution with identical <a href="https://en.wikipedia.org/wiki/Marginal_distribution">marginal distributions</a></li>
</ul>
</li>
</ul>
<h3 id="kurtosis">Kurtosis<a aria-hidden="true" class="anchor-heading icon-link" href="#kurtosis"></a></h3>
<ul>
<li>a staple of probability theory</li>
<li>It is a measure of dispersion (along with <code>variance</code>, <code>skewness</code> and <code>min/max values</code>)</li>
<li>Kurtosis is a measure of the "tailedness" of a distribution
<pre><code>- In other words, the sharpness of the peak of the bell curve
</code></pre>
<img src="/assets/images/2021-03-27-20-48-10.png"></li>
</ul>
<hr>
<p>The first step when analyzing any statistical analysis is what ways are they measuring the parameters. For instance if we are measuring popularity of websites have a certain date range, then we would have to know are you calculating that by how many visits the sites gets or are you talking about how much time is spent on each site, and so on by asking questions like this we understand better the true intentions of the author of the analysis</p>
<p>The way the overall proportion of coin flips settles down to 50% isn’t that fate favors tails to compensate for the heads that have already landed; it’s that those first ten flips become less and less important the more flips we make.</p>
<p>The null hypothesis, in executive bullet-point form:</p>
<ol>
<li>Run an experiment.</li>
<li>Suppose the null hypothesis is true, and let <code>p</code> be the probability (under that hypothesis) of getting results as extreme as those observed.</li>
<li>The number <code>p</code> is called the <em>p-value</em>.</li>
<li>If it is very small, rejoice; you get to say your results are statistically significant.</li>
<li>If it is large, concede that the null hypothesis has not been ruled out.</li>
</ol>
<p>“Statistically noticeable” or “statistically detectable” would be a better term than “statistically significant”</p>
<ul>
<li>That would be truer to the meaning of the method, which merely counsels us about the existence of an effect but is silent about its size or importance.</li>
</ul>
<p>The purpose of statistics isn’t to tell us what to believe, but to tell us what to do.
Statistics is about making decisions, not answering questions.</p>
<p>Pie charts should not have more than 3 variables included</p>
<ul>
<li>reasoning: when there are more than 3, our eyes start playing tricks and we are no longer able to draw conclusions from the visualization
<ul>
<li>verify this claim (originally from Layton) </li>
</ul>
</li>
</ul>
<p>Faking randomness</p>
<ul>
<li>When people try to purposely make something look random, it ends up being evidence that it wasn't a random process at all. Chunks occur naturally in a randomized process. When we try to fake it, we naturally avoid chunking. This is a clue that it was a faked process.</li>
<li>ex. 7738499 is more likely to be a random number than 3950681. the second example has no chunked numbers, and no repeated numbers. This looks deliberately crafted to seem random.</li>
</ul>
<h1 id="population-vs-sample">Population vs Sample<a aria-hidden="true" class="anchor-heading icon-link" href="#population-vs-sample"></a></h1>
<ul>
<li>population denotes all members of a group. 
<ul>
<li>spec: Depending on what we are looking at, and what we are interested in, within the stock market, a populate could be every single stock in existence, or it could be every stock in the S&#x26;P500. </li>
<li>populations are difficult to observe in real life
<ul>
<li>ex. how could we realistically obtain the data with every single student at a university included?</li>
</ul>
</li>
</ul>
</li>
<li>sample denotes that we are taking some subset of this population for analysis purposes</li>
<li>The goal of statistics is to gather and analyze data from a sample, that also serves as a good representation of the entire population </li>
<li>a good sample: each unit in the sample should have had equal % opportunity to enter the sample (from the population).
<ul>
<li>ex. if we wanted to get a sample of all students, and went and chose 50 people in the Gym, we are getting a bad sample, because the likelihood of any one person in the gym getting selected for the sample is higher than the likelihood of any one person in the library getting selected for the sample. We are not getting a good representation of the population</li>
</ul>
</li>
<li>Samples should be both 100% (ideally) random and representative of the population</li>
<li>Statistics are derived from samples, while Parameters are derived from populations
<ul>
<li>At a particular time, there may exist a Parameter for the percentage of all voters in USA who prefer Donald Trump, but we obviously can't ask everyone, so we need to take a sample. </li>
<li>Parameters are therefore more of an ephemeral value that can't actually be obtained in the current moment. We use samples to try and get as close to that parameter as we can. </li>
</ul>
</li>
</ul>
<h1 id="variance-standard-deviation-coefficient-of-variation">Variance, Standard Deviation, Coefficient of Variation<a aria-hidden="true" class="anchor-heading icon-link" href="#variance-standard-deviation-coefficient-of-variation"></a></h1>
<h3 id="variance">Variance<a aria-hidden="true" class="anchor-heading icon-link" href="#variance"></a></h3>
<ul>
<li>measures how far a set of numbers are spread out from their average value
<ul>
<li>a measure of dispersion of a datapoint around its mean</li>
</ul>
</li>
<li>each data point will be a certain distance away from the <strong>mean line</strong>. This is the variance of a given period</li>
<li>variance = sum of squared differences between observed values (the points on a plot) and the population mean (the straight line), divided by total # of observations 
<ul>
<li>in other words, variance is a function of how far those observed values are from the <strong>mean line</strong></li>
</ul>
</li>
<li>Mathematically, the <code>variance of the sample = variance of the population / sample size</code>
<ul>
<li>This is because as the sample size increases, sample means cluster more closely around the population mean.</li>
</ul>
</li>
</ul>
<h3 id="standard-deviation-σ">Standard Deviation (σ)<a aria-hidden="true" class="anchor-heading icon-link" href="#standard-deviation-σ"></a></h3>
<ul>
<li>equal to square root of <code>Variance</code></li>
<li>SD is more useful than variance, and variance's biggest value is in letting us easily calculate SD</li>
<li>SD is closer to the reality of what it is we are sampling.
<ul>
<li>ex. we have a sample of 10 stock prices and have calculated the <code>SD=3</code>. Since the units we used for the calculation was <code>$1</code>, the SD is also in terms of <code>$1</code> (ie. SD is $3) </li>
</ul>
</li>
<li>unable to compare different datasets with SD alone (need CV)</li>
</ul>
<h3 id="covariance-cov">Covariance (cov)<a aria-hidden="true" class="anchor-heading icon-link" href="#covariance-cov"></a></h3>
<ul>
<li>the relationship between two variables
<ul>
<li>ex. stock price</li>
</ul>
</li>
<li>covariance is not the same thing as correlation, but covariance is used in the formula to determine correlation</li>
<li>a higher covariance means the stocks tend to move together. Negative covariance means they have little correlation.
<ul>
<li>verify this. I suspect that negative value means "negative correlation", not "no correlation". This would make 0 "not correlated at all"</li>
</ul>
</li>
<li>like variance, covariance is not a standardized value
<ul>
<li>To get a more comparable value, we will need to convert to <code>correlation</code></li>
</ul>
</li>
<li>Modern Portfolio Theorists believe that the key is to build a portfolio composed of stocks that have low covariance. 
<ul>
<li>It is a way to remove non-systemic risk</li>
</ul>
</li>
</ul>
<h4 id="standard-error-se">Standard error (SE)<a aria-hidden="true" class="anchor-heading icon-link" href="#standard-error-se"></a></h4>
<ul>
<li>def - the SD between samples of a single population
<ul>
<li>spec: Therefore, we are getting an idea of how well we sampled the data. The more random and representative a piece of data is, the better sample it is. Therefore, if we got another sample from the same population and there was no variance, we would know we did a perfect job of getting a sample.</li>
</ul>
</li>
<li>usually "Standard Error" refers to "Standard Error of the Mean" (ie. we are measuring the "mean" statistic)</li>
<li>standard deviation is commonly used to measure confidence in statistical conclusions
<ul>
<li>ex. margin of error determined by calculating the expected standard deviation</li>
</ul>
</li>
</ul>
<h5 id="use">Use<a aria-hidden="true" class="anchor-heading icon-link" href="#use"></a></h5>
<ul>
<li>With a statistical observation (plotted on a graph), we would have applied formulas to make observations. When we know the SE, we are able to recalculate those formulas with the added benefit of having the SE. This effectively allows us to incorporate the effect of the SE into the function that we have used to generate (secondary) data.</li>
</ul>
<h4 id="coefficient-of-variation-cv">Coefficient of Variation (CV)<a aria-hidden="true" class="anchor-heading icon-link" href="#coefficient-of-variation-cv"></a></h4>
<ul>
<li>equal to <code>SD / mean</code></li>
<li>It simply exists to standardize the variation of a distribution.</li>
<li>With this number, we can compare the SD of different datasets
<ul>
<li>ex. we are looking at a restaurant menu with prices in both USD and CAD. Ultimately, The SD of each dataset will be different, but the CV will be identical between the datasets</li>
</ul>
</li>
<li>tells us the SD relative to the mean</li>
<li><a href="https://en.wikipedia.org/wiki/Coefficient_of_variation#Examples_of_misuse">pitfalls</a></li>
</ul>
<h1 id="distributions">Distributions<a aria-hidden="true" class="anchor-heading icon-link" href="#distributions"></a></h1>
<h2 id="normal-distribution">Normal Distribution<a aria-hidden="true" class="anchor-heading icon-link" href="#normal-distribution"></a></h2>
<p>A normal distribution of data is like rolling a pair of dice. We know that over many rolls, the most common result from the dice will be seven, and the least common results will be two and twelve.</p>
<ul>
<li>It is a characteristic of normal distributions that 1SD from the mean will cover 68% of cases, 2SD will cover 96% of cases, and 3SD will cover 99.9% of cases.</li>
</ul>
<h3 id="central-limit-theorem">Central Limit Theorem<a aria-hidden="true" class="anchor-heading icon-link" href="#central-limit-theorem"></a></h3>
<ul>
<li>as the sample size gets larger, the sampling distribution of the mean approaches a normal distribution</li>
<li>theorem holds especially true for samples sizes over 30</li>
<li>ex. if we take 2 die and roll them 1000 times, we will find that the each observed sum will form in the shape of a bell curve, with 7 being in the center and 14 and 2 being in the tails. </li>
</ul>
<h2 id="fat-tail-curves">Fat Tail Curves<a aria-hidden="true" class="anchor-heading icon-link" href="#fat-tail-curves"></a></h2>
<p>if we take A bell curve, we know that the extremes are predictable. we have a tough percentage probabilities that an event in the tail will come to pass. however, with fat tail, we have a situation where there are many unlikely extreme events. each event is still equally as unlikely, but there are a lot more of them, making an extreme outcome more likely than with a bell curve</p>
<ul>
<li>the more extreme events that are possible, the higher the probability that one of them will occur</li>
<li>explanation: think of a the heights of a population (normal distribution). there are outliers, but there is a cap on how extreme they will be. we will not see a man 10x the height of the average. but with a curve with fat tails, like wealth, we regularly meet people who are 1000s of times wealithier than the average</li>
<li>its a logical fallacy to compare statistics of normal distributions and fat tails </li>
<li>ex. there are twice as many deaths from slipping and falling on your head than there are from terrorist attacks. therefore we shouldn't be concerned. the problem with this statement is that slipping and falling is normal distribution, while terrorist attacks are fat tail. put another way, deaths from falling down stairs is pretty bounded. there wont be a 10x increase all of a sudden. the same cant be guaranteed for terrorist attacks</li>
<li>Fat tail is just a technical term for Black Swan Events</li>
</ul>
<h2 id="marginal-distribution">Marginal Distribution<a aria-hidden="true" class="anchor-heading icon-link" href="#marginal-distribution"></a></h2>
<ul>
<li>def - the probability distribution of the variables contained in a subset of data
<pre><code>- ex. given a sample of 100 students and their relation between the amount of time they study and their resulting grades, we can plot what these numbers look like. This is the marginal distribution
</code></pre>
<img src="/assets/images/2021-03-27-20-48-46.png">
<pre><code>- by adding up the values in the top row (8/200 + 2/100), we can easily determine that 5% of students get a grade of &#x3C;= 20%
</code></pre>
</li>
<li>Once we have this, we can use <strong>conditional distribution</strong> to answer questions like: "what is the probability a student studied for 10 hours and received a grade of 20% or less?"</li>
<li>ex. Imagine we are trying to determine the odds of getting hit by a car while crossing the street with a traffic light (at various colors). We will define 2 discrete random variables:
<ol>
<li>H={hit,not hit}</li>
<li>L={red,yellow,green}</li>
</ol>
<ul>
<li>H depends on L. In other words, your likelihood of getting hit are increased if L=green.</li>
<li>This means that for any given pair of H and L, we must consider the bivariate (joint) probability distribution of H and L. We need to determine the probability of that particular pair of events occurring if the pedestrian ignores the state of the light.</li>
<li>In trying to figure out the marginal probability of being hit <code>P(H=hit)</code>, what we are interested in is the probability that the pedestrian is hit, where the color of the light is unknown, and the pedestrian ignores the color of the light </li>
<li>In this example, we need to know the duration of time each light is on (ie. the probability of any one light being on), and we need to know the probability of getting hit by a car, given the color of the light</li>
</ul>
</li>
</ul>
<h3 id="why-we-square-the-differences">Why we square the differences<a aria-hidden="true" class="anchor-heading icon-link" href="#why-we-square-the-differences"></a></h3>
<ul>
<li>This serves 2 main purposes
<pre><code>1. Gives us a positive number
	- We square in situations where we are looking for *magnitude*, and don't care about the direction of that magnitude (ex. variance)
	- Also if we just add up the first numbers without squaring, we will net out at zero 
2. Amplifies the effect of large differences
</code></pre>
<img src="/assets/images/2021-03-27-20-49-07.png"></li>
</ul>
<h3 id="law-of-large-numbers">Law of large numbers<a aria-hidden="true" class="anchor-heading icon-link" href="#law-of-large-numbers"></a></h3>
<p>As individuals, people are completely unpredictable. Okay? One person making one bet... I couldn't possibly tell you what they're going to do. But the law of large numbers tells me that a million people making a million bets? That is completely predictable. Completely ordered.</p>
<hr>
<h2 id="pitfalls-with-statistics">Pitfalls with statistics<a aria-hidden="true" class="anchor-heading icon-link" href="#pitfalls-with-statistics"></a></h2>
<h3 id="non-linear-correlation">Non-linear correlation<a aria-hidden="true" class="anchor-heading icon-link" href="#non-linear-correlation"></a></h3>
<ul>
<li>Pearson's Correlation Coefficient only considers a linear relationship. for instance, if we are measuring the relationship between vitamin intake and health, it will be correlated to an extent, then will level off. applying pearsons correlation coefficient here will show the two as being only tenuously correlated
<ul>
<li><a href="https://en.wikipedia.org/wiki/Anscombe&#x27;s_quartet">related</a></li>
</ul>
</li>
</ul>
<hr>
<h3 id="type-i--type-ii-errors">Type I &#x26; Type II Errors<a aria-hidden="true" class="anchor-heading icon-link" href="#type-i--type-ii-errors"></a></h3>
<ul>
<li>Equivalent to False Positive (<code>type I</code>) and False Negative (<code>type II</code>)</li>
<li>These are the two kinds of errors in a binary test</li>
<li>Intuitively, type I errors can be thought of as errors of commission, and type II errors as errors of omission
<ul>
<li>ex. if we are shown a blurry picture and asked to identify it, an <em>error of commission</em> would be classifying the image as a dog, when it in fact isn't (<code>type I</code>). On the other hand, classifying the image as "not a dog", when it in fact is, would be an <em>error of omission</em>.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="confidence-interval">Confidence Interval<a aria-hidden="true" class="anchor-heading icon-link" href="#confidence-interval"></a></h3>
<p>if a confidence level is 90%, that means that in 90% of the samples taken (from a larger population), the interval estimate will contain the population parameter
confidence levels are determined before any analysis (so we don't make the analysis fit the data).
95% confidence level is most common</p>
<ul>
<li>Factors affecting the width of the confidence interval include the size of the sample, the confidence level, and the variability in the sample
interpreting</li>
<li>"Were this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 90%."</li>
<li>The confidence interval can be expressed in terms of a single sample: "There is a 90% probability that the calculated confidence interval from some future experiment encompasses the true value of the population parameter."</li>
<li>The explanation of a confidence interval can amount to something like: "The confidence interval represents values for the population parameter for which the difference between the parameter and the observed estimate is not statistically significant at the 10% level"
what it is <em>not</em></li>
<li>A 95% confidence level does not mean that for a given realized interval there is a 95% probability that the population parameter lies within the interval (i.e., a 95% probability that the interval covers the population parameter). once an interval is calculated, this interval either covers the parameter value or it does not; it is no longer a matter of probability. The 95% probability relates to the reliability of the estimation procedure, not to a specific calculated interval</li>
</ul>
<h3 id="correlation">Correlation<a aria-hidden="true" class="anchor-heading icon-link" href="#correlation"></a></h3>
<p>relationships bind facts together, satisfying our human desire to have a narrative explain events (rather than acknowledge that randomness occurs)</p>
<h3 id="univariate-analysis">Univariate analysis<a aria-hidden="true" class="anchor-heading icon-link" href="#univariate-analysis"></a></h3>
<p>univariate analysis is oversimplistic and fails to uncover the underlying reasons of why some phenomenon occurs
ex. take the gender pay gap. It's indisputable that in the aggregate, men get paid 9% more than women. However, this oversimplification ignores important factors in pay. For instance, individuals with the personality trait known as Agreeableness tend to get paid less. Women on average tend to be more agreeable than men. Not all women are more agreeable than men, and those women on average get paid more. What if it were the case that a woman and a man have exactly the same personality, and they got paid the same? would that dispel the idea that there is a legitimate gap, or would it just further underline the individual reasons that result in that gender pay gap?</p>
<ul>
<li>This underpins the issue with univariate analysis: you look at a single variable and attempt to understand the picture and all the reasons why that outcome has occurred. What you have failed to realize is that there may be 20 main factors that determine pay inequality, and gender and agreeableness might only constitute 2 of them.</li>
<li>Other underlying differences between women and men that may help explain the perceived pay gap: Women tend to be less inclined to take risks, armoire influenced by the chance of loss, or less competitive, and less status conscious.</li>
</ul>
<hr>
<strong>Children</strong>
<ol>
<li><a href="/notes/rU0dtUCoC7aKXM3nKbhmv">Average</a></li>
<li><a href="/notes/bXFCYJQhuzce02keksRnj">Bell Curve</a></li>
<li><a href="/notes/ugIWZMFABnVN8dJsmYgJu">Forecasting</a></li>
<li><a href="/notes/SlQM4igy2NbNBNBmhYZ0w">Graphs</a></li>
<li><a href="/notes/7RpCT3uJb4mUtXjldB6Vp">Probability</a></li>
<li><a href="/notes/z0xR9x1E9faoBlk2k0ogo">Regression</a></li>
<li><a href="/notes/ESke4ACtmSwkqkKhwGf8n">Sampling</a></li>
</ol>
<hr>
<strong>Backlinks</strong>
<ul>
<li><a href="/notes/8TJC5FfwKtfmH4GfOZVoO">Intelligence</a></li>
</ul></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#overview" title="Overview">Overview</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#approaches" title="Approaches">Approaches</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#descriptive-statistics" title="Descriptive Statistics">Descriptive Statistics</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#inferential-inductive-statistics" title="Inferential (Inductive) Statistics">Inferential (Inductive) Statistics</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#probability-distribution" title="Probability Distribution">Probability Distribution</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#discrete-probability-distribution" title="Discrete Probability Distribution">Discrete Probability Distribution</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#continuous-probability-distribution" title="Continuous Probability Distribution">Continuous Probability Distribution</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#univariatemultivariate-distribution" title="Univariate/Multivariate Distribution">Univariate/Multivariate Distribution</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#kurtosis" title="Kurtosis">Kurtosis</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#population-vs-sample" title="Population vs Sample">Population vs Sample</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#variance-standard-deviation-coefficient-of-variation" title="Variance, Standard Deviation, Coefficient of Variation">Variance, Standard Deviation, Coefficient of Variation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#variance" title="Variance">Variance</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#standard-deviation-σ" title="Standard Deviation (σ)">Standard Deviation (σ)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#covariance-cov" title="Covariance (cov)">Covariance (cov)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#standard-error-se" title="Standard error (SE)">Standard error (SE)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#use" title="Use">Use</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#coefficient-of-variation-cv" title="Coefficient of Variation (CV)">Coefficient of Variation (CV)</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#distributions" title="Distributions">Distributions</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#normal-distribution" title="Normal Distribution">Normal Distribution</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#central-limit-theorem" title="Central Limit Theorem">Central Limit Theorem</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#fat-tail-curves" title="Fat Tail Curves">Fat Tail Curves</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#marginal-distribution" title="Marginal Distribution">Marginal Distribution</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#why-we-square-the-differences" title="Why we square the differences">Why we square the differences</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#law-of-large-numbers" title="Law of large numbers">Law of large numbers</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#pitfalls-with-statistics" title="Pitfalls with statistics">Pitfalls with statistics</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#non-linear-correlation" title="Non-linear correlation">Non-linear correlation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#type-i--type-ii-errors" title="Type I &amp; Type II Errors">Type I &amp; Type II Errors</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#confidence-interval" title="Confidence Interval">Confidence Interval</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#correlation" title="Correlation">Correlation</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#univariate-analysis" title="Univariate analysis">Univariate analysis</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"kh7Y1bQw7Om31yurE4CrW","title":"Statistics","desc":"","updated":1636691742724,"created":1616903247412,"custom":{},"fname":"statistics","type":"note","vault":{"fsPath":"../main/thoughts-on","name":"thoughts-on"},"contentHash":"ca2fc5f4a08ddaacb7fede45c997bf7e","links":[{"from":{"fname":"intelligence","id":"8TJC5FfwKtfmH4GfOZVoO","vaultName":"thoughts-on"},"type":"backlink","position":{"start":{"line":4,"column":3,"offset":249},"end":{"line":4,"column":46,"offset":292},"indent":[]},"value":"statistics"}],"anchors":{"overview":{"type":"header","text":"Overview","value":"overview","line":9,"column":0,"depth":1},"approaches":{"type":"header","text":"Approaches","value":"approaches","line":10,"column":0,"depth":2},"descriptive-statistics":{"type":"header","text":"Descriptive Statistics","value":"descriptive-statistics","line":11,"column":0,"depth":3},"inferential-inductive-statistics":{"type":"header","text":"Inferential (Inductive) Statistics","value":"inferential-inductive-statistics","line":22,"column":0,"depth":3},"probability-distribution":{"type":"header","text":"Probability Distribution","value":"probability-distribution","line":30,"column":0,"depth":2},"discrete-probability-distribution":{"type":"header","text":"Discrete Probability Distribution","value":"discrete-probability-distribution","line":45,"column":0,"depth":3},"continuous-probability-distribution":{"type":"header","text":"Continuous Probability Distribution","value":"continuous-probability-distribution","line":52,"column":0,"depth":3},"univariatemultivariate-distribution":{"type":"header","text":"Univariate/Multivariate Distribution","value":"univariatemultivariate-distribution","line":62,"column":0,"depth":3},"kurtosis":{"type":"header","text":"Kurtosis","value":"kurtosis","line":67,"column":0,"depth":3},"population-vs-sample":{"type":"header","text":"Population vs Sample","value":"population-vs-sample","line":101,"column":0,"depth":1},"variance-standard-deviation-coefficient-of-variation":{"type":"header","text":"Variance, Standard Deviation, Coefficient of Variation","value":"variance-standard-deviation-coefficient-of-variation","line":115,"column":0,"depth":1},"variance":{"type":"header","text":"Variance","value":"variance","line":116,"column":0,"depth":3},"standard-deviation-σ":{"type":"header","text":"Standard Deviation (σ)","value":"standard-deviation-σ","line":126,"column":0,"depth":3},"covariance-cov":{"type":"header","text":"Covariance (cov)","value":"covariance-cov","line":133,"column":0,"depth":3},"standard-error-se":{"type":"header","text":"Standard error (SE)","value":"standard-error-se","line":144,"column":0,"depth":4},"use":{"type":"header","text":"Use","value":"use","line":151,"column":0,"depth":5},"coefficient-of-variation-cv":{"type":"header","text":"Coefficient of Variation (CV)","value":"coefficient-of-variation-cv","line":154,"column":0,"depth":4},"distributions":{"type":"header","text":"Distributions","value":"distributions","line":162,"column":0,"depth":1},"normal-distribution":{"type":"header","text":"Normal Distribution","value":"normal-distribution","line":163,"column":0,"depth":2},"central-limit-theorem":{"type":"header","text":"Central Limit Theorem","value":"central-limit-theorem","line":167,"column":0,"depth":3},"fat-tail-curves":{"type":"header","text":"Fat Tail Curves","value":"fat-tail-curves","line":172,"column":0,"depth":2},"marginal-distribution":{"type":"header","text":"Marginal Distribution","value":"marginal-distribution","line":180,"column":0,"depth":2},"why-we-square-the-differences":{"type":"header","text":"Why we square the differences","value":"why-we-square-the-differences","line":194,"column":0,"depth":3},"law-of-large-numbers":{"type":"header","text":"Law of large numbers","value":"law-of-large-numbers","line":202,"column":0,"depth":3},"pitfalls-with-statistics":{"type":"header","text":"Pitfalls with statistics","value":"pitfalls-with-statistics","line":207,"column":0,"depth":2},"non-linear-correlation":{"type":"header","text":"Non-linear correlation","value":"non-linear-correlation","line":208,"column":0,"depth":3},"type-i--type-ii-errors":{"type":"header","text":"Type I \u0026 Type II Errors","value":"type-i--type-ii-errors","line":214,"column":0,"depth":3},"confidence-interval":{"type":"header","text":"Confidence Interval","value":"confidence-interval","line":222,"column":0,"depth":3},"correlation":{"type":"header","text":"Correlation","value":"correlation","line":234,"column":0,"depth":3},"univariate-analysis":{"type":"header","text":"Univariate analysis","value":"univariate-analysis","line":237,"column":0,"depth":3}},"children":["rU0dtUCoC7aKXM3nKbhmv","bXFCYJQhuzce02keksRnj","ugIWZMFABnVN8dJsmYgJu","SlQM4igy2NbNBNBmhYZ0w","7RpCT3uJb4mUtXjldB6Vp","z0xR9x1E9faoBlk2k0ogo","ESke4ACtmSwkqkKhwGf8n"],"parent":"8qvi4S0Dk1BAaCLhfHqAp","data":{}},"body":"\u003ch1 id=\"statistics\"\u003eStatistics\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#statistics\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cp\u003eA person who doesn't stop to consider the assumptions of the techniques she's using is, in effect, thinking that her techniques are magical\u003c/p\u003e\n\u003ch1 id=\"overview\"\u003eOverview\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#overview\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"approaches\"\u003eApproaches\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#approaches\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"descriptive-statistics\"\u003eDescriptive Statistics\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#descriptive-statistics\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ea summary of data that quantitatively describes or summarizes features from a collection of information\u003c/li\u003e\n\u003cli\u003eaim is to summarize a sample\n\u003cul\u003e\n\u003cli\u003eIt is therefore parametric\n\u003cul\u003e\n\u003cli\u003eie. the analysis of the statistical set is based on parameters, like \u003ccode\u003evariance\u003c/code\u003e, or a \u003ccode\u003ecentral tendency\u003c/code\u003e (ie. mean, median, mode, range)\n\u003cul\u003e\n\u003cli\u003ea central tendency is often contrasted with its own level of variance\u003c/li\u003e\n\u003cli\u003eboth central tendencies and their levels of variance are considered main properties of a distribution. With them we may judge whether a set of data has a strong central tendency based on the level of variance that it has.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eex. \u003cem\u003emean\u003c/em\u003e, \u003cem\u003estandard deviation\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003espec: description statistics are backward looking, inductive statistics are forward looking\u003c/li\u003e\n\u003cli\u003edescriptive statistics does not rest on the assumption that the data came from a larger population\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"inferential-inductive-statistics\"\u003eInferential (Inductive) Statistics\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#inferential-inductive-statistics\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eaim is to learn about the population that the sample represents\n\u003cul\u003e\n\u003cli\u003eTherefore, it is developed on the basis of probability theory and by definition, cannot be parametric\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStatistical inference is the process of analyzing data to make inferences on an underlying\u003c/li\u003e\n\u003cli\u003eDescriptive stats do add valuable context when trying to analyze data inductively. \n\u003cul\u003e\n\u003cli\u003eex. in papers reporting on human subjects, typically a table is included giving the overall sample size, sample sizes in important subgroups, as well as demographic info like age, sex, ethnicity\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe conclusion of a statistical inference is called a \u003ccode\u003estatistical proposition\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"probability-distribution\"\u003eProbability Distribution\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#probability-distribution\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003edef - the mathematical function that tells us the probability that each possible outcome occurs. \u003c/li\u003e\n\u003cli\u003eIt is a mathematical representation of a random phenomenon\u003c/li\u003e\n\u003cli\u003eIt is a function which takes the \u003ccode\u003esample space\u003c/code\u003e (ie. all possible outcomes) as an input, and returns us the probability of their occurrence.\n\u003cul\u003e\n\u003cli\u003eex. of a coin flip, \u003ccode\u003esample space = {HEADS, TAILS}\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eIn other words, we give the function a single possible outcome, and the graph gives us the probability of that outcome occurring (the outcome of an event occurring is a subset of the sample space.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ethe probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory\u003c/li\u003e\n\u003cli\u003eProbability distributions are more preferable than simple numbers for describing a quantity because:\n\u003col\u003e\n\u003cli\u003eThere is spread or variability in almost any value that can be measured in a population\u003c/li\u003e\n\u003cli\u003ealmost all measurements are made with some intrinsic error\n\u003cul\u003e\n\u003cli\u003eex. in physics many processes are described probabilistically\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/List_of_probability_distributions\"\u003eCommon probability distributions\u003c/a\u003e:\n\u003cul\u003e\n\u003cli\u003enormal distribution, which is related to linear growth\u003c/li\u003e\n\u003cli\u003epareto distribution (pareto principle), which is related to exponential growth\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"discrete-probability-distribution\"\u003eDiscrete Probability Distribution\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#discrete-probability-distribution\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAll binomial distributions are discrete\n\u003cul\u003e\n\u003cli\u003ein other words, they are composed of discrete random variables\n\u003cul\u003e\n\u003cli\u003evariable here, refers to the potential outcomes (observations) that can occur\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe set of possible outcomes is discrete and predictable\n\u003cul\u003e\n\u003cli\u003eex. coin toss, dice roll\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"continuous-probability-distribution\"\u003eContinuous Probability Distribution\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#continuous-probability-distribution\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAll normal distribtions are continuous\u003c/li\u003e\n\u003cli\u003eThe set of possible outcomes can take place on a continuous range\n\u003cul\u003e\n\u003cli\u003eex. temperatures\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThese probabilities are described by a \u003ccode\u003eprobability density function\u003c/code\u003e (PDF)\n\u003cul\u003e\n\u003cli\u003eThis function takes in a single sample from the \u003ccode\u003esample space\u003c/code\u003e and returns us \"the probability that the input variable would equal that sample in real life\"\u003c/li\u003e\n\u003cli\u003eex. what is the probability that the temperature at 7:00PM, as measured in reality, is equal to the sample recorded in this sample?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ethe absolute likelihood for a continuous random variable to take on any particular value is 0\n\u003cul\u003e\n\u003cli\u003eThis is because there are an infinite set of possible values (ie. infinite sample space)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"univariatemultivariate-distribution\"\u003eUnivariate/Multivariate Distribution\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#univariatemultivariate-distribution\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eUnivariate - The probability distribution of only 1 random variable\u003c/li\u003e\n\u003cli\u003eMultivariate - The probability distribution of multiple random variables (a vector)\n\u003cul\u003e\n\u003cli\u003eregression towards the mean can be defined for any bivariate distribution with identical \u003ca href=\"https://en.wikipedia.org/wiki/Marginal_distribution\"\u003emarginal distributions\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"kurtosis\"\u003eKurtosis\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#kurtosis\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ea staple of probability theory\u003c/li\u003e\n\u003cli\u003eIt is a measure of dispersion (along with \u003ccode\u003evariance\u003c/code\u003e, \u003ccode\u003eskewness\u003c/code\u003e and \u003ccode\u003emin/max values\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eKurtosis is a measure of the \"tailedness\" of a distribution\n\u003cpre\u003e\u003ccode\u003e- In other words, the sharpness of the peak of the bell curve\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/images/2021-03-27-20-48-10.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003eThe first step when analyzing any statistical analysis is what ways are they measuring the parameters. For instance if we are measuring popularity of websites have a certain date range, then we would have to know are you calculating that by how many visits the sites gets or are you talking about how much time is spent on each site, and so on by asking questions like this we understand better the true intentions of the author of the analysis\u003c/p\u003e\n\u003cp\u003eThe way the overall proportion of coin flips settles down to 50% isn’t that fate favors tails to compensate for the heads that have already landed; it’s that those first ten flips become less and less important the more flips we make.\u003c/p\u003e\n\u003cp\u003eThe null hypothesis, in executive bullet-point form:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRun an experiment.\u003c/li\u003e\n\u003cli\u003eSuppose the null hypothesis is true, and let \u003ccode\u003ep\u003c/code\u003e be the probability (under that hypothesis) of getting results as extreme as those observed.\u003c/li\u003e\n\u003cli\u003eThe number \u003ccode\u003ep\u003c/code\u003e is called the \u003cem\u003ep-value\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eIf it is very small, rejoice; you get to say your results are statistically significant.\u003c/li\u003e\n\u003cli\u003eIf it is large, concede that the null hypothesis has not been ruled out.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e“Statistically noticeable” or “statistically detectable” would be a better term than “statistically significant”\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThat would be truer to the meaning of the method, which merely counsels us about the existence of an effect but is silent about its size or importance.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe purpose of statistics isn’t to tell us what to believe, but to tell us what to do.\nStatistics is about making decisions, not answering questions.\u003c/p\u003e\n\u003cp\u003ePie charts should not have more than 3 variables included\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ereasoning: when there are more than 3, our eyes start playing tricks and we are no longer able to draw conclusions from the visualization\n\u003cul\u003e\n\u003cli\u003everify this claim (originally from Layton) \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFaking randomness\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhen people try to purposely make something look random, it ends up being evidence that it wasn't a random process at all. Chunks occur naturally in a randomized process. When we try to fake it, we naturally avoid chunking. This is a clue that it was a faked process.\u003c/li\u003e\n\u003cli\u003eex. 7738499 is more likely to be a random number than 3950681. the second example has no chunked numbers, and no repeated numbers. This looks deliberately crafted to seem random.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"population-vs-sample\"\u003ePopulation vs Sample\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#population-vs-sample\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003epopulation denotes all members of a group. \n\u003cul\u003e\n\u003cli\u003espec: Depending on what we are looking at, and what we are interested in, within the stock market, a populate could be every single stock in existence, or it could be every stock in the S\u0026#x26;P500. \u003c/li\u003e\n\u003cli\u003epopulations are difficult to observe in real life\n\u003cul\u003e\n\u003cli\u003eex. how could we realistically obtain the data with every single student at a university included?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003esample denotes that we are taking some subset of this population for analysis purposes\u003c/li\u003e\n\u003cli\u003eThe goal of statistics is to gather and analyze data from a sample, that also serves as a good representation of the entire population \u003c/li\u003e\n\u003cli\u003ea good sample: each unit in the sample should have had equal % opportunity to enter the sample (from the population).\n\u003cul\u003e\n\u003cli\u003eex. if we wanted to get a sample of all students, and went and chose 50 people in the Gym, we are getting a bad sample, because the likelihood of any one person in the gym getting selected for the sample is higher than the likelihood of any one person in the library getting selected for the sample. We are not getting a good representation of the population\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSamples should be both 100% (ideally) random and representative of the population\u003c/li\u003e\n\u003cli\u003eStatistics are derived from samples, while Parameters are derived from populations\n\u003cul\u003e\n\u003cli\u003eAt a particular time, there may exist a Parameter for the percentage of all voters in USA who prefer Donald Trump, but we obviously can't ask everyone, so we need to take a sample. \u003c/li\u003e\n\u003cli\u003eParameters are therefore more of an ephemeral value that can't actually be obtained in the current moment. We use samples to try and get as close to that parameter as we can. \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"variance-standard-deviation-coefficient-of-variation\"\u003eVariance, Standard Deviation, Coefficient of Variation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#variance-standard-deviation-coefficient-of-variation\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch3 id=\"variance\"\u003eVariance\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#variance\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003emeasures how far a set of numbers are spread out from their average value\n\u003cul\u003e\n\u003cli\u003ea measure of dispersion of a datapoint around its mean\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eeach data point will be a certain distance away from the \u003cstrong\u003emean line\u003c/strong\u003e. This is the variance of a given period\u003c/li\u003e\n\u003cli\u003evariance = sum of squared differences between observed values (the points on a plot) and the population mean (the straight line), divided by total # of observations \n\u003cul\u003e\n\u003cli\u003ein other words, variance is a function of how far those observed values are from the \u003cstrong\u003emean line\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMathematically, the \u003ccode\u003evariance of the sample = variance of the population / sample size\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eThis is because as the sample size increases, sample means cluster more closely around the population mean.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"standard-deviation-σ\"\u003eStandard Deviation (σ)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#standard-deviation-σ\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eequal to square root of \u003ccode\u003eVariance\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eSD is more useful than variance, and variance's biggest value is in letting us easily calculate SD\u003c/li\u003e\n\u003cli\u003eSD is closer to the reality of what it is we are sampling.\n\u003cul\u003e\n\u003cli\u003eex. we have a sample of 10 stock prices and have calculated the \u003ccode\u003eSD=3\u003c/code\u003e. Since the units we used for the calculation was \u003ccode\u003e$1\u003c/code\u003e, the SD is also in terms of \u003ccode\u003e$1\u003c/code\u003e (ie. SD is $3) \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eunable to compare different datasets with SD alone (need CV)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"covariance-cov\"\u003eCovariance (cov)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#covariance-cov\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ethe relationship between two variables\n\u003cul\u003e\n\u003cli\u003eex. stock price\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ecovariance is not the same thing as correlation, but covariance is used in the formula to determine correlation\u003c/li\u003e\n\u003cli\u003ea higher covariance means the stocks tend to move together. Negative covariance means they have little correlation.\n\u003cul\u003e\n\u003cli\u003everify this. I suspect that negative value means \"negative correlation\", not \"no correlation\". This would make 0 \"not correlated at all\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003elike variance, covariance is not a standardized value\n\u003cul\u003e\n\u003cli\u003eTo get a more comparable value, we will need to convert to \u003ccode\u003ecorrelation\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eModern Portfolio Theorists believe that the key is to build a portfolio composed of stocks that have low covariance. \n\u003cul\u003e\n\u003cli\u003eIt is a way to remove non-systemic risk\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"standard-error-se\"\u003eStandard error (SE)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#standard-error-se\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003edef - the SD between samples of a single population\n\u003cul\u003e\n\u003cli\u003espec: Therefore, we are getting an idea of how well we sampled the data. The more random and representative a piece of data is, the better sample it is. Therefore, if we got another sample from the same population and there was no variance, we would know we did a perfect job of getting a sample.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eusually \"Standard Error\" refers to \"Standard Error of the Mean\" (ie. we are measuring the \"mean\" statistic)\u003c/li\u003e\n\u003cli\u003estandard deviation is commonly used to measure confidence in statistical conclusions\n\u003cul\u003e\n\u003cli\u003eex. margin of error determined by calculating the expected standard deviation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"use\"\u003eUse\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#use\"\u003e\u003c/a\u003e\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eWith a statistical observation (plotted on a graph), we would have applied formulas to make observations. When we know the SE, we are able to recalculate those formulas with the added benefit of having the SE. This effectively allows us to incorporate the effect of the SE into the function that we have used to generate (secondary) data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"coefficient-of-variation-cv\"\u003eCoefficient of Variation (CV)\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#coefficient-of-variation-cv\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eequal to \u003ccode\u003eSD / mean\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eIt simply exists to standardize the variation of a distribution.\u003c/li\u003e\n\u003cli\u003eWith this number, we can compare the SD of different datasets\n\u003cul\u003e\n\u003cli\u003eex. we are looking at a restaurant menu with prices in both USD and CAD. Ultimately, The SD of each dataset will be different, but the CV will be identical between the datasets\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003etells us the SD relative to the mean\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Coefficient_of_variation#Examples_of_misuse\"\u003epitfalls\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"distributions\"\u003eDistributions\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#distributions\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003ch2 id=\"normal-distribution\"\u003eNormal Distribution\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#normal-distribution\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eA normal distribution of data is like rolling a pair of dice. We know that over many rolls, the most common result from the dice will be seven, and the least common results will be two and twelve.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt is a characteristic of normal distributions that 1SD from the mean will cover 68% of cases, 2SD will cover 96% of cases, and 3SD will cover 99.9% of cases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"central-limit-theorem\"\u003eCentral Limit Theorem\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#central-limit-theorem\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eas the sample size gets larger, the sampling distribution of the mean approaches a normal distribution\u003c/li\u003e\n\u003cli\u003etheorem holds especially true for samples sizes over 30\u003c/li\u003e\n\u003cli\u003eex. if we take 2 die and roll them 1000 times, we will find that the each observed sum will form in the shape of a bell curve, with 7 being in the center and 14 and 2 being in the tails. \u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"fat-tail-curves\"\u003eFat Tail Curves\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#fat-tail-curves\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eif we take A bell curve, we know that the extremes are predictable. we have a tough percentage probabilities that an event in the tail will come to pass. however, with fat tail, we have a situation where there are many unlikely extreme events. each event is still equally as unlikely, but there are a lot more of them, making an extreme outcome more likely than with a bell curve\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe more extreme events that are possible, the higher the probability that one of them will occur\u003c/li\u003e\n\u003cli\u003eexplanation: think of a the heights of a population (normal distribution). there are outliers, but there is a cap on how extreme they will be. we will not see a man 10x the height of the average. but with a curve with fat tails, like wealth, we regularly meet people who are 1000s of times wealithier than the average\u003c/li\u003e\n\u003cli\u003eits a logical fallacy to compare statistics of normal distributions and fat tails \u003c/li\u003e\n\u003cli\u003eex. there are twice as many deaths from slipping and falling on your head than there are from terrorist attacks. therefore we shouldn't be concerned. the problem with this statement is that slipping and falling is normal distribution, while terrorist attacks are fat tail. put another way, deaths from falling down stairs is pretty bounded. there wont be a 10x increase all of a sudden. the same cant be guaranteed for terrorist attacks\u003c/li\u003e\n\u003cli\u003eFat tail is just a technical term for Black Swan Events\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"marginal-distribution\"\u003eMarginal Distribution\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#marginal-distribution\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003edef - the probability distribution of the variables contained in a subset of data\n\u003cpre\u003e\u003ccode\u003e- ex. given a sample of 100 students and their relation between the amount of time they study and their resulting grades, we can plot what these numbers look like. This is the marginal distribution\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/images/2021-03-27-20-48-46.png\"\u003e\n\u003cpre\u003e\u003ccode\u003e- by adding up the values in the top row (8/200 + 2/100), we can easily determine that 5% of students get a grade of \u0026#x3C;= 20%\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003eOnce we have this, we can use \u003cstrong\u003econditional distribution\u003c/strong\u003e to answer questions like: \"what is the probability a student studied for 10 hours and received a grade of 20% or less?\"\u003c/li\u003e\n\u003cli\u003eex. Imagine we are trying to determine the odds of getting hit by a car while crossing the street with a traffic light (at various colors). We will define 2 discrete random variables:\n\u003col\u003e\n\u003cli\u003eH={hit,not hit}\u003c/li\u003e\n\u003cli\u003eL={red,yellow,green}\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eH depends on L. In other words, your likelihood of getting hit are increased if L=green.\u003c/li\u003e\n\u003cli\u003eThis means that for any given pair of H and L, we must consider the bivariate (joint) probability distribution of H and L. We need to determine the probability of that particular pair of events occurring if the pedestrian ignores the state of the light.\u003c/li\u003e\n\u003cli\u003eIn trying to figure out the marginal probability of being hit \u003ccode\u003eP(H=hit)\u003c/code\u003e, what we are interested in is the probability that the pedestrian is hit, where the color of the light is unknown, and the pedestrian ignores the color of the light \u003c/li\u003e\n\u003cli\u003eIn this example, we need to know the duration of time each light is on (ie. the probability of any one light being on), and we need to know the probability of getting hit by a car, given the color of the light\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"why-we-square-the-differences\"\u003eWhy we square the differences\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#why-we-square-the-differences\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThis serves 2 main purposes\n\u003cpre\u003e\u003ccode\u003e1. Gives us a positive number\n\t- We square in situations where we are looking for *magnitude*, and don't care about the direction of that magnitude (ex. variance)\n\t- Also if we just add up the first numbers without squaring, we will net out at zero \n2. Amplifies the effect of large differences\n\u003c/code\u003e\u003c/pre\u003e\n\u003cimg src=\"/assets/images/2021-03-27-20-49-07.png\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"law-of-large-numbers\"\u003eLaw of large numbers\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#law-of-large-numbers\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eAs individuals, people are completely unpredictable. Okay? One person making one bet... I couldn't possibly tell you what they're going to do. But the law of large numbers tells me that a million people making a million bets? That is completely predictable. Completely ordered.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"pitfalls-with-statistics\"\u003ePitfalls with statistics\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pitfalls-with-statistics\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"non-linear-correlation\"\u003eNon-linear correlation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#non-linear-correlation\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePearson's Correlation Coefficient only considers a linear relationship. for instance, if we are measuring the relationship between vitamin intake and health, it will be correlated to an extent, then will level off. applying pearsons correlation coefficient here will show the two as being only tenuously correlated\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Anscombe\u0026#x27;s_quartet\"\u003erelated\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"type-i--type-ii-errors\"\u003eType I \u0026#x26; Type II Errors\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#type-i--type-ii-errors\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEquivalent to False Positive (\u003ccode\u003etype I\u003c/code\u003e) and False Negative (\u003ccode\u003etype II\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eThese are the two kinds of errors in a binary test\u003c/li\u003e\n\u003cli\u003eIntuitively, type I errors can be thought of as errors of commission, and type II errors as errors of omission\n\u003cul\u003e\n\u003cli\u003eex. if we are shown a blurry picture and asked to identify it, an \u003cem\u003eerror of commission\u003c/em\u003e would be classifying the image as a dog, when it in fact isn't (\u003ccode\u003etype I\u003c/code\u003e). On the other hand, classifying the image as \"not a dog\", when it in fact is, would be an \u003cem\u003eerror of omission\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"confidence-interval\"\u003eConfidence Interval\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#confidence-interval\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eif a confidence level is 90%, that means that in 90% of the samples taken (from a larger population), the interval estimate will contain the population parameter\nconfidence levels are determined before any analysis (so we don't make the analysis fit the data).\n95% confidence level is most common\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFactors affecting the width of the confidence interval include the size of the sample, the confidence level, and the variability in the sample\ninterpreting\u003c/li\u003e\n\u003cli\u003e\"Were this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 90%.\"\u003c/li\u003e\n\u003cli\u003eThe confidence interval can be expressed in terms of a single sample: \"There is a 90% probability that the calculated confidence interval from some future experiment encompasses the true value of the population parameter.\"\u003c/li\u003e\n\u003cli\u003eThe explanation of a confidence interval can amount to something like: \"The confidence interval represents values for the population parameter for which the difference between the parameter and the observed estimate is not statistically significant at the 10% level\"\nwhat it is \u003cem\u003enot\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eA 95% confidence level does not mean that for a given realized interval there is a 95% probability that the population parameter lies within the interval (i.e., a 95% probability that the interval covers the population parameter). once an interval is calculated, this interval either covers the parameter value or it does not; it is no longer a matter of probability. The 95% probability relates to the reliability of the estimation procedure, not to a specific calculated interval\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"correlation\"\u003eCorrelation\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#correlation\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003erelationships bind facts together, satisfying our human desire to have a narrative explain events (rather than acknowledge that randomness occurs)\u003c/p\u003e\n\u003ch3 id=\"univariate-analysis\"\u003eUnivariate analysis\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#univariate-analysis\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eunivariate analysis is oversimplistic and fails to uncover the underlying reasons of why some phenomenon occurs\nex. take the gender pay gap. It's indisputable that in the aggregate, men get paid 9% more than women. However, this oversimplification ignores important factors in pay. For instance, individuals with the personality trait known as Agreeableness tend to get paid less. Women on average tend to be more agreeable than men. Not all women are more agreeable than men, and those women on average get paid more. What if it were the case that a woman and a man have exactly the same personality, and they got paid the same? would that dispel the idea that there is a legitimate gap, or would it just further underline the individual reasons that result in that gender pay gap?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThis underpins the issue with univariate analysis: you look at a single variable and attempt to understand the picture and all the reasons why that outcome has occurred. What you have failed to realize is that there may be 20 main factors that determine pay inequality, and gender and agreeableness might only constitute 2 of them.\u003c/li\u003e\n\u003cli\u003eOther underlying differences between women and men that may help explain the perceived pay gap: Women tend to be less inclined to take risks, armoire influenced by the chance of loss, or less competitive, and less status conscious.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cstrong\u003eChildren\u003c/strong\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"/notes/rU0dtUCoC7aKXM3nKbhmv\"\u003eAverage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/bXFCYJQhuzce02keksRnj\"\u003eBell Curve\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/ugIWZMFABnVN8dJsmYgJu\"\u003eForecasting\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/SlQM4igy2NbNBNBmhYZ0w\"\u003eGraphs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/7RpCT3uJb4mUtXjldB6Vp\"\u003eProbability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/z0xR9x1E9faoBlk2k0ogo\"\u003eRegression\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/notes/ESke4ACtmSwkqkKhwGf8n\"\u003eSampling\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003cstrong\u003eBacklinks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/notes/8TJC5FfwKtfmH4GfOZVoO\"\u003eIntelligence\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","noteIndex":{"id":"8qvi4S0Dk1BAaCLhfHqAp","title":"Second Brain","desc":"","updated":1672111562730,"created":1616360437888,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"../main/thoughts-on","name":"thoughts-on"},"contentHash":"dd394dade3f08feaa5cf4d1caeb1c991","links":[],"anchors":{"tags":{"type":"header","text":"Tags","value":"tags","line":12,"column":0,"depth":3}},"children":["8aPvLrwcPnxcTwx7PdvXW","l9ldUruxqRBZwMij5KlS5","xWh3SS0J1hvRSZ7g4LvEE","cc5kdn5ay8omhcud9hjfpyk","9jNuGCXVuFaZOGZfLpKi5","mhlpKRrfT0qlXzTaapFYZ","QU1tbQ4KX64UBCWtCxAcR","U4tQLU5tRHqr9e7H3G4Gm","PvcIkEJcDtcv7luF2mBIU","2wW5Kxk794H9mfnu9VPA8","iwANjkbi3nbIovlVpepWm","XYgvsgvjzgoyvY6KAEu5Y","pT9wnWnS42pFId9UIH5he","gmXZ6Xy8f8FFQ66Z5DYqh","CjV6QXVHIqQTuZTSlSSlB","IgbLitAU1k0ioGhuvmjjS","D42jFbx0vNlT7S6QRb5ee","dFs5zopMmrh3rPgQaSkyP","W0AgEfoTzJ8VxVE4QqhSL","ggU4P3K1OhUlHBJbpgHiD","vtu2srTzCfoNUHv0Xcbxn","6kuwlrc3PT5JoQS9xZWZW","qoZ6e9KsYPestYFpO4Ce4","GPH8BieqB4dtqul80gJvX","MbW1wF5mIZ7g3DPXrOSyZ","Ay9vOAmvV8ODJFpaEkw6J","8Pi3wPs4LTxlAEOUE48Te","SDd8268l79j2cnvCVVl8F","zI5rK8mFkNghzLgbQD6ox","zCwMY5ckgKAqstOhw2Drd","i5Fya3Vzzm0rBT0ctByuo","YDbL8QXchj03NvTIZOdrk","km4fy13hx9puopyjr1ajdgj","WdAyuDkIi16pl1pnaopzx","15ax6k6x106sotobkyn3jqp","sgu2QxqDGup93hJPx8RJd","mT663nw5DUEaEGzXI98yD","Dyg6bFtxE0UkB7gz6pqzq","cn1x9vhdapyt1fvgtvfekea","0aYhvzCfaIxLaNhqNCV9Q","q9NskCGxFGNQwBDcsgNkY","wAdoBPUMHEFJkm6XVlHXW","sCYBuXnSNA259MDwhakmv","YgQusR9D6RbBgdkLuJuDh","ovHaNSb3tnl2LTdV7wwau","eqklm05q2g46ikqaqv9ijh9","8TJC5FfwKtfmH4GfOZVoO","9rKIOn3BV7ZU1FFlJxmvC","rsrE9vBXK0ehNkbEjltlZ","XTkKLi0Jg1imZ2lbhgOrv","4Fuw6OHQ5uhEVCUfXDfkb","pUEyedPYYlNSVLuiUSnge","KtoCw0HFEddGJ7GuW7GnN","BoxZ36iWEbUoRProxPo4D","gCmclfAHAt1tEcFDc24eD","TNhxyx7GEdGDM0PBI08b1","guyW7WWQNDGDALFV7tO57","YeEWZtb7FWvYTyJwF7zhR","DUrXQvolf76ILMKJm4Zt6","Jo4jAQcwgxsdxkcxUceR4","X6yPFUKIjs6tENQ3uNzE2","aRSv8qoJLyeE6xDAAUYpU","jwb0toyj23byc7krttp0l8k","ztqrUgUYssC0FAWDGZxWF","d7evrjgli013nul3mosquea","mvBLIZW424WZ5mCmjKQaB","YoV6c5CmpHYNEuYU4PsgH","JQzF2iPfQ2897j6HFCoY9","TicwTJD2oaTpncpPy93s2","KiGGs6PmvIlIt50r2tF2O","4TiqSFRd4hD0e5tyHbf4d","8F9H6GIButHtrFKXHzZFu","t5CaAEDQFRutGrV0ru97d","het26IMKUnKB7npru8tG4","GgRKYXklrXEAs9u15sOM9","PPvKHQNulUEvXEitqAZxM","SMGOfSTElTkJBezhNvkZ6","358nLIVsR9zmItfuBCWq2","xewZxeuhzI7qMvoYdcnLM","FAAtgk8cbuownxuv0Vlou","64N1mdh3JWL1WOv1dilYD","aCd3q2zSzgQnHKi1vOJFj","kh7Y1bQw7Om31yurE4CrW","5162x506d5mgwkn07plcuo3","LGnyEtwA1LlztJJwEpg6l","zoJRWH59vBCpqnm8fOpis","xlg9pngfxviby5z2qfcfpcv","G6zDrHf3Ua8R7vllalBc5","NrjfBxxDU92ByYSIJjgDM","zNcMG5nj1t69zkAFnvn5S","JIHQPGBUDqAEPzbhfFxlc","MmjHaByLtYx5sv2HzXnWd"],"parent":null,"data":{},"body":"\nWelcome to my Second Brain! Here you'll find thoughts, lessons, facts and any other interesting musings that I've come across or thought up over the years. A lot of what I have gathered over the years are originally from the minds of others, while some are my own. Some may be outdated and no longer reflect my current views. Some may contain misinformation, though I try to be rigorous with my additions. You can think of it as my personal wiki.\n\nThis Dendron vault is the sister component to the [Tech-focused Digital Garden](https://tech.kyletycholiz.com)\n\n### Tags\nThroughout the Second Brain, I have made use of tags, which give semantic meaning to the pieces of information.\n\n- `ex.` - Denotes an *example* of the preceding piece of information\n- `spec:` - Specifies that the preceding information has some degree of *speculation* to it, and may not be 100% factual. Ideally this gets clarified over time as my understanding develops.\n- `anal:` - Denotes an *analogy* of the preceding information. Often I will attempt to link concepts to others that I have previously learned.\n- `mn:` - Denotes a *mnemonic*\n- `expl:` - Denotes an *explanation*"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":false,"leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2,"vaultSelectionModeOnCreate":"smart"}},"insertNote":{"initialValue":"templates"},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"randomNote":{},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"dendronVersion":"0.83.0","vaults":[{"fsPath":"../main/thoughts-on","name":"thoughts-on"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":true,"maxPreviewsCached":10,"maxNoteLength":204800,"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"enableUserTags":true,"enableHashTags":true,"enableEditorDecorations":true,"enableFullHierarchyNoteTitle":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enableMermaid":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Dendron","description":"The non-Tech Second Brain of Kyle Tycholiz"},"github":{"cname":"thoughts.kyletycholiz.com","enableEditLink":true,"editLinkText":"Edit this page on GitHub","editBranch":"master","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","enableMermaid":true,"siteUrl":"https://thoughts.kyletycholiz.com","duplicateNoteBehavior":{"action":"useVault","payload":["thoughts-on"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"kh7Y1bQw7Om31yurE4CrW"},"buildId":"Ev72apsD0Krsz6dJLOqfG","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>